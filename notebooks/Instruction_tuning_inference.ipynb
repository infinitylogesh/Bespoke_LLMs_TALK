{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference notebook for Evol Instruction tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadCustomModel(\n",
       "      (transformer): GPT2CustomModel(\n",
       "        (wte): Embedding(49280, 2048)\n",
       "        (wpe): Embedding(2048, 2048)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x GPT2CustomBlock(\n",
       "            (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2MQAttention(\n",
       "              (q_attn): Conv1D()\n",
       "              (kv_attn): Conv1D()\n",
       "              (c_proj): Linear(\n",
       "                in_features=2048, out_features=2048, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Linear(\n",
       "                in_features=8192, out_features=2048, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): FastGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=49280, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import PeftModel,get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftConfig\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "#if args.n_gpu > 0:\n",
    "    #torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "peft_model_id = \"santa_instruct_model/\"\n",
    "model_name_or_path = \"bigcode/santacoder\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "org_model = AutoModelForCausalLM.from_pretrained(model_name_or_path,trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(org_model, peft_model_id)\n",
    "model.eval()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "DEFAULT_PAD_TOKEN = \"<|pad|>\"\n",
    "DEFAULT_EOS_TOKEN = \"<|endoftext|>\"\n",
    "DEFAULT_UNK_TOKEN = \"<|unk|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict,\n",
    "    tokenizer,\n",
    "    model,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "\n",
    "        \n",
    "special_tokens_dict = dict()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ids = [tokenizer.encode(stop_word) for stop_word in [\"<|endoftext|>\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model,input_text):\n",
    "    inputs = tokenizer(input_text,return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=512,do_sample=True,temperature=0.6)\n",
    "    \n",
    "    return tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a python code to validate an email address using Regular Expressions\n",
    "\n",
    "#### Instruction tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request\n",
      "### Instruction:\n",
      "Create a python code to validate an email address using Regular Expressions\n",
      "\n",
      "### Response:Validate an email address using regular expressions:\n",
      "\n",
      "`import re`\n",
      "\n",
      "email_address = 'ychag@example.com'\n",
      "\n",
      "# Match against the email address's regular expression\n",
      "match = re.match(r'[^@]+@[^@]+\\.[^@]+', email_address)\n",
      "\n",
      "# Print the match if it exists\n",
      "if match:\n",
      "    print(match.group())\n",
      "else:\n",
      "    print('Invalid email address')\n",
      "\n",
      "\n",
      "# Explanations:\n",
      "# - `re.match()` will match the email address's regular expression against thestring `email_address`\n",
      "# - The groups will be returned by the match object.\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Create a python code to validate an email address using Regular Expressions\"\n",
    "\n",
    "input_text = PROMPT_DICT['prompt_no_input'].format(instruction=instruction)\n",
    " \n",
    "predictions = infer(model,input_text)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Create a python code to validate an email address using Regular Expressions\n",
      "# You can use RegEx's to validate email address\n",
      "# RegEx Patterns are composed of one or more characters that can be matched\n",
      "# The first character can be any of:\n",
      "# a - A letter\n",
      "# b - A number\n",
      "# l - A lower case letter\n",
      "# u - An upper case letter\n",
      "# d - A dollar sign\n",
      "#. - A dot\n",
      "# - - A hyphen\n",
      "# # - A hash\n",
      "# @ - A at sign\n",
      "# $ - A dollar sign\n",
      "# & - A ampersand\n",
      "# * - A times sign\n",
      "# () - An open parenthesis\n",
      "# ) - A close parenthesis\n",
      "\n",
      "import re\n",
      "\n",
      "# The email address is composed of a user id, a dot, a domain name, a comma\n",
      "# and a space. This is not normally a valid email address. But it does\n",
      "# seem to be a valid email address.\n",
      "\n",
      "email = 'user@domain.com,,, '\n",
      "\n",
      "# Use re.compile to create a RegEx\n",
      "regex = re.compile(\n",
      "    r'user@domain\\.com\\,?,?'\n",
      ")\n",
      "\n",
      "# Search the email for matches\n",
      "matches = regex.search(email)\n",
      "\n",
      "# The first match is returned\n",
      "print(matches.group())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "org_model.eval()\n",
    "\n",
    "input_text = \"\"\"# Create a python code to validate an email address using Regular Expressions\"\"\"\n",
    " \n",
    "predictions = infer(org_model,input_text)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add the missing symbols and brackets for the for loop. for x range 0, 5 print(x)\n",
    "\n",
    "#### Instruction tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request\n",
      "### Instruction:\n",
      "Add the missing symbols and brackets for the for loop. for x range 0, 5 print(x)\n",
      "\n",
      "### Response:Add the missing symbols and brackets for the for loop.\n",
      "for x in range(0, 5):\n",
      "    print(x)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Add the missing symbols and brackets for the for loop. for x range 0, 5 print(x)\"\n",
    "\n",
    "input_text = PROMPT_DICT['prompt_no_input'].format(instruction=instruction)\n",
    " \n",
    "predictions = infer(model,input_text)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Add the missing symbols and brackets for the for loop. for x range 0, 5 print(x)\n",
      "for x in range(0, 5):\n",
      "    print(x)\n",
      "\n",
      "# Add the missing symbols and brackets for the while loop.\n",
      "# while x < 5:\n",
      "#     print(x)\n",
      "#     x += 1\n",
      "\n",
      "# Add the missing symbols and brackets for the forloop with range().\n",
      "# for x in range(0, 5):\n",
      "#     print(x)\n",
      "\n",
      "# Add the missing symbols and brackets for the whileloop with range().\n",
      "# while x < 5:\n",
      "#     print(x)\n",
      "#     x += 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "org_model.eval()\n",
    "\n",
    "input_text = \"\"\"# Add the missing symbols and brackets for the for loop. for x range 0, 5 print(x)\"\"\"\n",
    " \n",
    "predictions = infer(org_model,input_text)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the correct syntax to create a dictionary in Python?\n",
    "\n",
    "#### Instruction tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request\n",
      "### Instruction:\n",
      "What is the correct syntax to create a dictionary in Python?\n",
      "\n",
      "### Response:Dictionary in Python is a collection of key-value pairs that can be used for storing data. For example, if you wanted to store data about a particular person, you could create a dictionary that stores name, email and phone. Or you could store it for every person in your dataset.\n",
      "\n",
      "You can create a dictionary from scratch:\n",
      "\n",
      "```python\n",
      "person_dict = {\n",
      "    \"name\": \"Mary\",\n",
      "    \"email\": \"dycjh@example.com\",\n",
      "    \"phone\": \"1234567890\"\n",
      "}\n",
      "\n",
      "print(person_dict) # Should be {\"name\": \"Mary\", \"email\": \"dycjh@example.com\", \"phone\": \"1234567890\"}\n",
      "\n",
      "# Alternatively, you could create a dictionary from another dictionary:\n",
      "\n",
      "another_person_dict = {\n",
      "    \"name\": \"John\",\n",
      "    \"email\": \"envkt@example.com\",\n",
      "    \"phone\": \"1234567890\"\n",
      "}\n",
      "\n",
      "person_dict = another_person_dict\n",
      "print(person_dict) # Should be {\"name\": \"John\", \"email\": \"envkt@example.com\", \"phone\": \"1234567890\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction = \"What is the correct syntax to create a dictionary in Python?\"\n",
    "\n",
    "input_text = PROMPT_DICT['prompt_no_input'].format(instruction=instruction)\n",
    " \n",
    "predictions = infer(model,input_text)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# What is the correct syntax to create a dictionary in Python?\n",
      "# We can use this syntax as follows:\n",
      "\n",
      "my_dict = {}\n",
      "my_dict[\"key1\"] = \"value1\"\n",
      "my_dict[\"key2\"] = \"value2\"\n",
      "\n",
      "print(my_dict)\n",
      "\n",
      "# Alternatively, we can use the key-value pair syntax:\n",
      "\n",
      "my_dict = {\"key1\": \"value1\", \"key2\": \"value2\"}\n",
      "\n",
      "print(my_dict)\n",
      "\n",
      "# In other words, the key-value pair syntax is preferred.\n",
      "\n",
      "# In other words, you can create dictionaries with key-value pairs or with key-value\n",
      "# pairs and lists as values, which would give you the same result:\n",
      "\n",
      "my_dict = {\"key1\": \"value1\", \"key2\": \"value2\"}\n",
      "my_list = [1, 2, 3]\n",
      "\n",
      "my_dict = {\"key1\": \"value1\", \"key2\": \"value2\"}\n",
      "my_list = [1, 2, 3]\n",
      "\n",
      "my_dict = {\"key1\": \"value1\", \"key2\": \"value2\"}\n",
      "my_list = [1, 2, 3]\n",
      "\n",
      "# In the end, you will see that we have gone from three key-value pairs to three\n",
      "# lists, and vice versa.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "org_model.eval()\n",
    "\n",
    "input_text = \"\"\"# What is the correct syntax to create a dictionary in Python?\"\"\"\n",
    " \n",
    "predictions = infer(org_model,input_text)\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_4bit",
   "language": "python",
   "name": "llama_4bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
